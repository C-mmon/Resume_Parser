{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ae75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8f2617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_path(pdf_path):\n",
    "    return extract_text(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e33de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "515cc8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone Number\n",
      "9701427755\n",
      "Email Address\n",
      "dush1729@gmail.com\n",
      "Name\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from tika import parser # pip install tika\n",
    "\n",
    "\n",
    "os.environ['CLASSPATH'] = \"./stanford-ner/stanford-ner.jar\"\n",
    "os.environ['STANFORD_MODELS'] = './sta/edu/stanford/nlp/models/ner'\n",
    "\n",
    "\n",
    "java_path = \"/usr/bin/java\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "#Phone Number Regex\n",
    "#works only for indian phone number\n",
    "PHONE_REG = re.compile(r'''(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})\n",
    "''', re.VERBOSE)\n",
    "\n",
    "#Email Regex\n",
    "email_Reg=re.compile(r'''((?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\]))''',re.VERBOSE)\n",
    "\n",
    "\n",
    "#Name implemented from stanford nerd\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    raw = parser.from_file('./sample.pdf')\n",
    "    text=(raw['content'])\n",
    "    mo = PHONE_REG.search(text)\n",
    "    print(\"Phone Number\")        \n",
    "    print(mo.group())\n",
    "    print(\"Email Address\")\n",
    "    em=email_Reg.search(text)\n",
    "    print(em.group())\n",
    "    print(\"Name\")\n",
    "    stanford_classifier = './sta/edu/stanford/nlp/models/ner/english.all.3class.caseless.distsim.crf.ser.gz'\n",
    "    st = StanfordNERTagger(stanford_classifier)\n",
    "    tagged = st.tag(str(text).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "269cd450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arijit--roy\n"
     ]
    }
   ],
   "source": [
    "#Extracting LinkedIn Profile Name\n",
    "result = text.find('inkedin.com/in')\n",
    "text_2=text[result+15:result+100]\n",
    "text_3=\"\"\n",
    "for i in text_2:\n",
    "    if i in \"\\n\":\n",
    "        break\n",
    "    else:\n",
    "        text_3+=i\n",
    "print(text_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e7b051ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radioactive11\n"
     ]
    }
   ],
   "source": [
    "#Extracting Github Profile Name\n",
    "result = text.find('github.com/')\n",
    "text_2=text[result+11:result+100]\n",
    "text_3=\"\"\n",
    "for i in text_2:\n",
    "    if i in \"\\n\" or i==' ':\n",
    "        break\n",
    "    else:\n",
    "        text_3+=i\n",
    "print(text_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ff06edc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aniket/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#Removing the stop keyword\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc=nlp(text)\n",
    "import  pandas as pd\n",
    "data=pd.read_csv(\"skills.csv\")\n",
    "SKILLS_DB=['CAT']\n",
    "df=pd.DataFrame(data)\n",
    "for index,row in df.iterrows():\n",
    "    SKILLS_DB.append(row['Skills'])\n",
    "\n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    "\n",
    "    nlp_text=nlp(text)\n",
    "    skillset=[]\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token in SKILLS_DB:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    return skillset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a2c9b833",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Intelligence',\n",
       " 'C++',\n",
       " 'C',\n",
       " 'Python',\n",
       " 'MySQL',\n",
       " 'PostgreSQL',\n",
       " 'OpenCV',\n",
       " 'Django',\n",
       " 'Flask',\n",
       " 'Azure',\n",
       " 'AWS',\n",
       " 'Unix',\n",
       " 'TMA',\n",
       " 'TMA',\n",
       " 'Flask',\n",
       " 'REST',\n",
       " 'API',\n",
       " 'Project',\n",
       " 'Digital',\n",
       " 'Health',\n",
       " 'Bridge',\n",
       " 'Features',\n",
       " 'Cancer',\n",
       " 'Physics',\n",
       " 'Chemistry',\n",
       " 'Mathematics',\n",
       " 'Cyber',\n",
       " 'Microsoft',\n",
       " 'CSE',\n",
       " 'Alexa',\n",
       " 'OpenCV']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_skills(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d3c8d4ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avatar_url': 'https://avatars.githubusercontent.com/u/26817752?v=4',\n",
      " 'bio': 'MLOps | AI | Computer Vision | Backend | Student Ambassador '\n",
      "        '@microsoft | \\r\\n'\n",
      "        'President @randomizemuj|',\n",
      " 'blog': 'www.radioactive11.codes',\n",
      " 'company': None,\n",
      " 'created_at': '2017-03-31T12:47:29Z',\n",
      " 'email': None,\n",
      " 'events_url': 'https://api.github.com/users/radioactive11/events{/privacy}',\n",
      " 'followers': 44,\n",
      " 'followers_url': 'https://api.github.com/users/radioactive11/followers',\n",
      " 'following': 10,\n",
      " 'following_url': 'https://api.github.com/users/radioactive11/following{/other_user}',\n",
      " 'gists_url': 'https://api.github.com/users/radioactive11/gists{/gist_id}',\n",
      " 'gravatar_id': '',\n",
      " 'hireable': True,\n",
      " 'html_url': 'https://github.com/radioactive11',\n",
      " 'id': 26817752,\n",
      " 'location': 'New Delhi,India',\n",
      " 'login': 'radioactive11',\n",
      " 'name': 'Arijit Roy',\n",
      " 'node_id': 'MDQ6VXNlcjI2ODE3NzUy',\n",
      " 'organizations_url': 'https://api.github.com/users/radioactive11/orgs',\n",
      " 'public_gists': 5,\n",
      " 'public_repos': 69,\n",
      " 'received_events_url': 'https://api.github.com/users/radioactive11/received_events',\n",
      " 'repos_url': 'https://api.github.com/users/radioactive11/repos',\n",
      " 'site_admin': False,\n",
      " 'starred_url': 'https://api.github.com/users/radioactive11/starred{/owner}{/repo}',\n",
      " 'subscriptions_url': 'https://api.github.com/users/radioactive11/subscriptions',\n",
      " 'twitter_username': '_radioactive11_',\n",
      " 'type': 'User',\n",
      " 'updated_at': '2021-07-04T13:12:00Z',\n",
      " 'url': 'https://api.github.com/users/radioactive11'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "# github username\n",
    "username = \"x4nth055\"\n",
    "# url to request\n",
    "url = f\"https://api.github.com/users/{text_3}\"\n",
    "# make the request and return the json\n",
    "user_data = requests.get(url).json()\n",
    "# pretty print JSON data\n",
    "pprint(user_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "499951bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', [pattern])\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4d958ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"    Current enrollment in a Bachelorâ€™s degree program in Computer Engineering, Information Technology or a related major.One or more years of coding experience in Java or Node.js. Experience building scalable web services with RESTful APIs.  Knowledge of database technologies.A degree in Computer Science or a related field.Experience in real-time communication products.Knowledge of infrastructure services such as Zookeeper and caching products such as Couchbase.Experience in scaling large high transaction volume web services.Examples of contributions to open source project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "768f66b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Technology', 'Java', 'Node.js', 'Couchbase']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_skills(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a591eed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-212-3e2f6552f8de>:4: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  df = json_normalize('./cleaned_related_skills(1).json')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-3e2f6552f8de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson_normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./cleaned_related_skills(1).json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./exported_json_data.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarning_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malternative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# adding deprecated directive to the docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/io/json/_normalize.py\u001b[0m in \u001b[0;36m_json_normalize\u001b[0;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrecord_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0;31m# naive normalization, this is idempotent for flat records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;31m# and potentially will inflate the data considerably for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/io/json/_normalize.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrecord_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0;31m# naive normalization, this is idempotent for flat records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;31m# and potentially will inflate the data considerably for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "df.to_excel('./exported_json_data.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0fce1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
